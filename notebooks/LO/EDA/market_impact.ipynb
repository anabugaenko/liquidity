{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc056dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from itertools import product\n",
    "from typing import Dict, Tuple\n",
    "from IPython.display import display\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from liquidity.features import compute_aggregate_features\n",
    "\n",
    "from market_impact.util.utils import normalize_imbalances\n",
    "from market_impact.response_functions import aggregate_impact\n",
    "from market_impact.util.plotting.constants import EBAY_COLORS\n",
    "from market_impact.finite_scaling.fss import mapout_scale_factors, transform\n",
    "from market_impact.finite_scaling.fit import fit_scaling_form, fit_scaling_law\n",
    "from market_impact.util.plotting.plot import plot_scaling_function, plot_collapsed_scaling_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d0ef60d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Package dependencies:\n",
    "# https://github.com/anabugaenko/liquidity\n",
    "# https://github.com/anabugaenko/market_impact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42886995",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Automatically reload changes in package dependencies\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96617c00",
   "metadata": {},
   "source": [
    "<!--\n",
    "Copyright 2022 Kenji Harada\n",
    "-->\n",
    "# Finitie-size scaling analysis\n",
    "The statistics of price returns have been purported to exhibit robust scaling dynamics resembling those of systems in critical states  where the maximum increments of asset returns were previously shown to be limited only by a second upper cutoff due to, for example, the finite size of the system. We perform a finite-size scaling analysis  of the signed conditional aggregate impact of an order and characterize the critical exponents of relevant observables such as price returns signs and signed volume imbalances. In this notebook, we introduce a FSS based on the method of Leas-squares, see:\n",
    "\n",
    "- Patzelt, F. and Bouchaud, J.P., 2018. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a179c201",
   "metadata": {},
   "source": [
    "## Finitie-size scaling of non-equilibrium phenomena\n",
    "\n",
    "\n",
    "Upon introduction of properly scaled variables, the finite-size scaling (FSS) analysis extracts numerical values for critical exponents that describe a given critical phenomenon in a finitie-size system. More formally, the scaling assumption states that if a physical quantity is considered to obey scaling, it can be expressed as \n",
    "$$ \n",
    "y\\left(x, T\\right) = T^{\\chi} \\mathscr{F}\\left(xT^{-\\varkappa}\\right), \n",
    "$$\n",
    "where x is a variable describing a physical system of which size is T, and exponents $\\chi$ and $\\varkappa$ are critical exponents. $\\mathscr{F}(\\cdot)$ is a scaling function which too exhibits universality. Then the FSS analysis is an inference of critical parameters so that if we introduce new variables\n",
    "$$ \n",
    "X \\equiv xT^{-\\kappa}, Y \\equiv y /T^{\\chi} =  yT^\\chi \n",
    "$$\n",
    "then the FSS law rewrites as \n",
    "$$\n",
    "Y = \\mathscr{F}(x)\n",
    "$$\n",
    "such that the data points collapse onto a single scaling function. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a8f12b5",
   "metadata": {},
   "source": [
    "## Infering the scaling function from data \n",
    "Because we may not know the form $\\mathscr{F}$ $\\textit{a prior}$ on grounds of theory, the statistical inference problem is acute and one has to assume both values of critical exponents as well as the form of the scaling function itself. Although $\\mathscr{F}(ùë•)$ can in principal function of virtually any form, Patzelt and Bouchaud (2018) and Farmer, Gerig and Lillo (2008) find it is well appoximated by a sigmoidal:\n",
    "\\begin{align*}\n",
    "\\mathscr{F}(x) = \\frac{x}{\\left(1 + | x |^\\alpha \\right)^{\\beta / \\alpha}},\n",
    "\\end{align*}\n",
    "which describes empirical observations for signed aggregate impact, where $\\alpha$ and $\\beta$ regulate the shape (steepness and symmetry) of $\\mathscr{F}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a9d9322",
   "metadata": {},
   "source": [
    "## Empirical scaling of price returns\n",
    "We are particualrly interested in the scaling laws governing the form of conditional aggregate price returns \n",
    "\\begin{align*}\n",
    "R\\left(\\Delta \\mathcal{E}, T\\right) \\cong R_T \\cdot \\mathscr{F}\\left(\\frac{\\Delta \\mathcal{E}}{\\mathcal{E}_T}\\right).\n",
    "\\end{align*}\n",
    "where and $R_T $ and $\\mathcal{E}_T$ are unknown scaling factors that do not depend on ùë•, but instead on the system size T. In fact, without imposing any assumptions, empiricism suggests a scaling law of the form\n",
    "\\begin{align*}\n",
    "\\mathcal{E}_T  \\thicksim \\mathcal{E}_DT^\\varkappa, \\\\\n",
    "    R_T \\thicksim \\mathcal{R}(1)T^\\chi, \n",
    "\\end{align*}\n",
    "which yields the following scaling law\n",
    "\\begin{align*}\n",
    "R \\left(\\Delta \\mathcal{E}, T \\right) = \\mathcal{R}(1)T^\\chi \\cdot \\mathscr{F}\\left(\\frac{\\Delta \\mathcal{E}}{\\mathcal{E}_DT^\\varkappa}\\right),\n",
    "\\end{align*}\n",
    "for aggregate price returns, where $\\mathcal{R}(1)$ and $\\mathcal{E}_D$ represent constants of unit dimension that define a characteristic length scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb477e7c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30c8a809",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants \n",
    "DATA_RANGE = list(range(5, 151))\n",
    "BINNING_FREQUENCIES = [5, 10, 20, 50, 100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d7f52c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load orderbook raw sample data\n",
    "stocks = ['TSLA', 'AMZN', 'NFLX', 'MSFT', 'EBAY', 'AAPL', 'GOOG']\n",
    "\n",
    "current_dir = os.path.abspath('.')\n",
    "root_dir = os.path.join(current_dir, '..', '..')\n",
    "data_dir = os.path.join(root_dir, 'data', 'market_orders')\n",
    "\n",
    "stock_data = {}\n",
    "\n",
    "# Loop through each stock\n",
    "for stock in stocks:\n",
    "\n",
    "    filename = f\"{stock}-2017-NEW.csv\"\n",
    "\n",
    "    stock_file_path = os.path.join(data_dir, filename)\n",
    "\n",
    "    # Read the CSV and store in the dictionary\n",
    "    stock_data[stock] = pd.read_csv(stock_file_path)\n",
    "\n",
    "# Access the dataframe using stock's ticker as key\n",
    "tsla_raw_df = stock_data['TSLA']\n",
    "amzn_raw_df = stock_data['AMZN']\n",
    "nflx_raw_df = stock_data['NFLX']\n",
    "msft_raw_df = stock_data['MSFT']\n",
    "ebay_raw_df = stock_data['EBAY']\n",
    "aapl_raw_df = stock_data['AAPL']\n",
    "goog_raw_df = stock_data['GOOG']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a7865d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "aapl_raw_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2a23b92",
   "metadata": {},
   "source": [
    "### Aggregate features \n",
    "We first coarse-grain the data into different binning frequencies T that represent different system sizes (in event time) by marginalize over microscopic degrees of freedom in the system to yield an effective coarse-grained description at long distances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51bb4291",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute aggregate features \n",
    "aggregate_features = compute_aggregate_features(aapl_raw_df.head(1000000), DATA_RANGE)\n",
    "display(aggregate_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fee7ed1e",
   "metadata": {},
   "source": [
    "### Aggregate imapct\n",
    "From aggegate features, we compute aggregate impact of market orders MO. In preprartion for FSS analysis., all impact data are automatically rescaled  rescaled each day by the corresponding values of $R(1)$ and the daily number $\\mathcal{E}_D$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a27ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute data for susceptibility\n",
    "imbalance_column = \"sign_imbalance\"\n",
    "aggregate_impact_data = aggregate_impact(aggregate_features, conditional_variable=imbalance_column)\n",
    "\n",
    "display(aggregate_impact_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "256f68b5",
   "metadata": {},
   "source": [
    "## Find shape parameters\n",
    "Determine the shape parameters $\\alpha$ and $\\beta$ of scaling function $\\mathscr{F}(\\cdot)$ by fitting the</b>\n",
    "scaling function for $\\textit{all}$ T."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31dbfad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare original data for fitting\n",
    "t_values = aggregate_impact_data['T'].values\n",
    "imbalance_values = aggregate_impact_data[imbalance_column].values\n",
    "r_values = aggregate_impact_data['R'].values\n",
    "\n",
    "# Fit data for all Ts\n",
    "params = fit_scaling_form(t_values, imbalance_values, r_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c33ed93e",
   "metadata": {},
   "outputs": [],
   "source": [
    "RT, VT, alpha, beta = params\n",
    "print(f'RT: {RT}')\n",
    "print(f'VT: {VT}')\n",
    "print(f'alpha: {alpha}')\n",
    "print(f'beta: {beta}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d66f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_scaling_function(\n",
    "    aggregate_impact_data, \n",
    "    scaling_params=params,\n",
    "    line_color=EBAY_COLORS.dark_color,\n",
    "    markers_color=\"white\", \n",
    "    plotting_func=\"scaling_form\",\n",
    "    imbalance_column=imbalance_column,\n",
    "    binning_frequencies=BINNING_FREQUENCIES)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87a37ba4",
   "metadata": {},
   "source": [
    "## Map-out scale factors\n",
    "Once $\\mathscr{F}(\\cdot)$ is fixed, we can use the found $\\alpha$ and $\\beta$ to map out the scale factors as a function of T (i.e., for each system size T), which are well very approximated by power-laws of T."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01108765",
   "metadata": {},
   "outputs": [],
   "source": [
    "RT_series, VT_series, RT_fit_object, VT_fit_object = mapout_scale_factors(aggregate_impact_data, alpha=alpha, beta=beta, imbalance_column=imbalance_column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "771fe5b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot scale factors RN and QN\n",
    "plt.scatter(RT_series['x_values'], RT_series['y_values'])\n",
    "plt.scatter(VT_series['x_values'], VT_series['y_values'])\n",
    "plt.loglog()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d284b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "RT_fit_object.powerlaw.fit_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cda412e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "VT_fit_object.powerlaw.fit_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87b36542",
   "metadata": {},
   "source": [
    "### Determine rescaling exponents\n",
    "In order to determine the rescaling exponents $\\chi$ and $\\varkappa$, the shape of the scaling form is fitted for each T keeping the same/constant values of $\\alpha$ and $\\beta$,</b> which are well approximated by the power law."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d77c42aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "chi = RT_fit_object.powerlaw.params.alpha\n",
    "kappa = VT_fit_object.powerlaw.params.alpha\n",
    "print(chi)\n",
    "print(kappa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "184bef38",
   "metadata": {},
   "outputs": [],
   "source": [
    "RT_fit_object.powerlaw.plot_fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ece7f00f",
   "metadata": {},
   "source": [
    "## Do FSS by method of Least-squares\n",
    "We can now use the found empirical scaling law to perform the FSS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b45b63f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare original data for fitting\n",
    "t_values = aggregate_impact_data['T'].values\n",
    "imbalance_values = aggregate_impact_data[imbalance_column].values\n",
    "r_values = aggregate_impact_data['R'].values\n",
    "\n",
    "# Fit data for all Ts\n",
    "params = fit_scaling_law(t_values, imbalance_values, r_values, reflect_y=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "148f56da",
   "metadata": {},
   "outputs": [],
   "source": [
    "chi, kappa, alpha, beta, CONST = params\n",
    "print(f'chi: {chi}')\n",
    "print(f'kappa: {kappa}')\n",
    "print(f'alpha: {alpha}')\n",
    "print(f'beta: {beta}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0362dd77",
   "metadata": {},
   "source": [
    "We plot the scaling for different binning_frequencies. Aggregate impact after an order \"appears\" to grow linear in volume imbalance with increasing $T$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0695e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_scaling_function(\n",
    "    aggregate_impact_data, \n",
    "    scaling_params=params,\n",
    "    line_color=EBAY_COLORS.dark_color,\n",
    "    markers_color=\"white\", \n",
    "    imbalance_column=imbalance_column,\n",
    "    binning_frequencies=BINNING_FREQUENCIES)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "167627a0",
   "metadata": {},
   "source": [
    "### Transform data\n",
    "To do the fss by method of least-squares, we use optimized critical paramters to rescale the scaling function onto a single master curve by initially fitting the scaling law to all $T$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4825c478",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0642c3d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform original data using found rescaling exponents chi ùõò and kapp œ∞\n",
    "rescaled_data = transform(aggregate_impact_data, rescaling_params=params, imbalance_column=imbalance_column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29475dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_collapsed_scaling_function(\n",
    "    rescaled_data,  \n",
    "    scaling_params=params,\n",
    "    line_color=EBAY_COLORS.dark_color, \n",
    "    markers_color=\"white\", \n",
    "    imbalance_column=imbalance_column, \n",
    "    master_curve=\"Sigmoid\",\n",
    "    binning_frequencies=BINNING_FREQUENCIES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daab4b9f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
